package sakura.test

import com.homework.da.listener.{BatchMonitorListener, CongestionMonitor}
import org.apache.spark.SparkConf
import org.apache.spark.streaming.{CongestionMonitorListener, Seconds, StreamingContext}
import org.fire.spark.streaming.core.Logging
import org.fire.spark.streaming.core.plugins.kafka.{KafkaDirectSource, SakuraKafkaDirectSource}

object KafkaTest extends Logging{
  def main(args: Array[String]) {

    // Create the context with a 1 second batch size
    val sparkConf = new SparkConf().setAppName("TraceTest").setMaster("local[3]")
    sparkConf.set("spark.source.kafka.consumer.topics", "test")
    sparkConf.set("spark.source.kafka.consumer.group.id", "test.t1")
    sparkConf.set("spark.source.kafka.consumer.bootstrap.servers", "localhost:9092")
    sparkConf.set("spark.source.kafka.consumer.auto.offset.reset", "latest")
    sparkConf.set("spark.source.kafka.consumer.key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
    sparkConf.set("spark.source.kafka.consumer.value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
sparkConf.set("spark.source.kafka.consumer.max.partition.fetch.bytes","10485760")
    sparkConf.set("spark.alert.ding","https://oapi.dingtalk.com/robot/send?access_token=dec3b1a3afb0b9f48985c35f0edc8ab373948e50d9d3630382a893f6f99fff03")
    sparkConf.set("spark.alert.mail","shaofengfeng@zuoyebang.com")
    sparkConf.set("spark.alert.mobile","17150012018")
    sparkConf.set("spark.run.main","KafkaTest")


    val ssc = new StreamingContext(sparkConf, Seconds(3))
    ssc.addStreamingListener(new CongestionMonitor(ssc))
    val sc = ssc.sparkContext
    // Create a socket stream on target ip:port and count the
    // words in input stream of \n delimited text (eg. generated by 'nc')
    // Note that no duplication in storage level only for running locally.
    // Replication necessary in distributed scenario for fault tolerance.

    val source: SakuraKafkaDirectSource[String, String] = new SakuraKafkaDirectSource[String, String](ssc)
    val lines = source.getDStream(_.value())

    val bytes:Array[Byte]=new Array[Byte](1024*1024*300);
    val accum = ssc.sparkContext.longAccumulator("Id Accumulator")

    lines.foreachRDD((rdd, time) => {
      rdd.repartition(100)
      val c=bytes.size
      log.warn(s"sakura:${rdd.count}")
      Thread.sleep(1000*5)
    })
    ssc.start()
    ssc.awaitTermination()
  }


}
