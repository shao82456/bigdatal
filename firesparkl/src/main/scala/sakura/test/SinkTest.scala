//package sakura.test
//
//import com.homework.da.format.{Header, RealTimeData}
//import com.homework.da.realtimeSink.KafkaSink
//import org.apache.spark.SparkConf
//import org.apache.spark.rdd.RDD
//import org.apache.spark.storage.StorageLevel
//import org.apache.spark.streaming.{Seconds, StreamingContext}
//
///**
// * Author: shaoff
// * Date: 2020/2/13 16:05
// * Package: sakura.test
// * Description:
// *
// */
//object SinkTest {
//  def main(args: Array[String]) {
//
//    // Create the context with a 1 second batch size
//    val sparkConf = new SparkConf().setAppName("SinkTest").setMaster("local[3]")
//
//    val ssc = new StreamingContext(sparkConf, Seconds(15))
//    val sc = ssc.sparkContext
//    // Create a socket stream on target ip:port and count the
//    // words in input stream of n delimited text (eg. generated by 'nc')
//    // Note that no duplication in storage level only for running locally.
//    // Replication necessary in distributed scenario for fault tolerance.
//    val accum = sc.longAccumulator("Id Accumulator")
//
//    val sink = new KafkaSink[String](sc, Map(
//      "bootstrap.servers" -> "localhost:9092",
//      "topic" -> "test",
//      "key.serializer" -> "org.apache.kafka.common.serialization.StringSerializer",
//      "value.serializer" -> "org.apache.kafka.common.serialization.StringSerializer")
//    )
//
//    val lines = ssc.socketTextStream("localhost", 8890, StorageLevel.MEMORY_AND_DISK_SER)
//
//    //    val revertFunc=(in:(String, Map[String, Any]))=>in._1
//    //    val e= lines2.map(x=>x.map(revertFunc))
//
//    lines.foreachRDD((rdd, time) => {
//      /*
//      上游数据如果是json格式（无论是否带header)，直接RealTimeData.parse[T]解析
//      如果不是json格式，自己解析后，调用RealTimeData.from[T]
//      */
//      val rtRDD = rdd.map(x => {
//        RealTimeData.from[String](x, Header())
//      })
//
//      rtRDD.map(rt=>{
//        val h=rt.header
//        val b=rt.body
//        val nb=b.toUpperCase()
//
//      })
//
//      val wordsRDD: RDD[RealTimeData[String]] = rtRDD.flatMap(_.flatMapBody(_.split(" ")))
//
//      //模拟处理，这里只是把上游数据转成大写的
//      val upperWordsRDD:RDD[RealTimeData[String]]=wordsRDD.map(_.mapBody(_.toUpperCase))
//
//      //sink到kafka
//      sink.output(upperWordsRDD, time)
//    })
//    ssc.start()
//    ssc.awaitTermination()
//  }
//
//  /*val wc:RDD[RealTimeData] = wordsRDD.map(rt => rt.map(word => (word, 1)))
//        .map(rt => (rt.body, rt))
//        .reduceByKey((a, b) => RealTimeData(a.header, (a.body._1, a.body._2 + b.body._2)))*/
//  //        rdd.flatMap(_.map())
//  //        .map(RealTimeData.from[String](_))
//
//}
