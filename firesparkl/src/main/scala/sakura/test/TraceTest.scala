package sakura.test

import java.util.UUID

import com.homework.da.log.{ElasticTracing, TracedData}
import org.apache.spark.SparkConf
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.dstream.DStream
import org.apache.spark.streaming.{Seconds, StreamingContext}

object TraceTest {
  def main(args: Array[String]) {

    // Create the context with a 1 second batch size
    val sparkConf = new SparkConf().setAppName("TraceTest").setMaster("local[3]")

    val ssc = new StreamingContext(sparkConf, Seconds(10))
    val sc = ssc.sparkContext
    // Create a socket stream on target ip:port and count the
    // words in input stream of \n delimited text (eg. generated by 'nc')
    // Note that no duplication in storage level only for running locally.
    // Replication necessary in distributed scenario for fault tolerance.
    val lines: DStream[String] = ssc.socketTextStream("localhost", 8890, StorageLevel.MEMORY_AND_DISK_SER)

    sparkConf.set("spark.trace.kafka.bootstrap.servers", "localhost:9092")
    sparkConf.set("spark.trace.kafka.topic", "test")

    val esTrace = new ElasticTracing(sparkConf)
    lines.foreachRDD((rdd, time) => {
      val words = rdd.flatMap(_.split(" "))
      //定义转换函数
      val convert = (word: String) => TracedData(UUID.randomUUID().toString, word, "v1")
      //trace words
      val tracedRdd = esTrace.transTrace[String](words, convert, "trace_test_input")
      val res = tracedRdd.map(word => (word, 1)).reduceByKey(_ + _)
      val tracedRes = esTrace.transTrace[(String, Int)](res,
        pair => TracedData(UUID.randomUUID().toString, pair, pair._1),
        "trace_test_output")
      println(tracedRes.collect().mkString("\n"))
    })
    ssc.start()
    ssc.awaitTermination()
  }
}
