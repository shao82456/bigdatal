package sakura.streaming

import java.util.UUID

import com.homework.da.format.DataTable
import com.homework.da.log.{ElasticTracing, TracedData}
import org.apache.spark.streaming.StreamingContext
import org.fire.spark.streaming.core.FireStreaming
import org.fire.spark.streaming.core.plugins.kafka.KafkaDirectSource

import scala.util.Try

object TraceTest extends FireStreaming {
  override def handle(ssc: StreamingContext): Unit = {
    val conf = ssc.sparkContext.getConf
    // Create a socket stream on target ip:port and count the
    // words in input stream of \n delimited text (eg. generated by 'nc')
    // Note that no duplication in storage level only for running locally.
    // Replication necessary in distributed scenario for fault tolerance.
    val source = new KafkaDirectSource[String, String](ssc)
    val lines = source.getDStream(_.value()).flatMap(line => {
      Try(Some(DataTable.str2dt(line))).getOrElse(None)
    })
    //    sparkConf.set("spark.trace.kafka.bootstrap.servers", "localhost:9092")
    //    sparkConf.set("spark.trace.kafka.topic", "test")

    val esTrace = new ElasticTracing(conf)
    lines.foreachRDD((rdd, time) => {
      //定义转换函数
      val convert = (word: String) => TracedData(UUID.randomUUID().toString, word, "v1")
      //trace words
      val tracedRdd = esTrace.transTrace[DataTable](rdd, dt => TracedData(UUID.randomUUID().toString,
        dt, dt.getNewValues().getOrElse("id", UUID.randomUUID()).toString,
        dt.TABLE, dt.DATABASE, dt.BINLOG_NAME
      ), "trace_test_input")
      val res = tracedRdd.filter(dt => dt.TIME.toLong % 2 == 0)
      val tracedRes = esTrace.transTrace[DataTable](res,
        dt => TracedData(UUID.randomUUID().toString, dt, dt.getNewValues().getOrElse("id", UUID.randomUUID()).toString),
        "trace_test_output")
      println(tracedRes.count())
      source.updateOffsets(time.milliseconds)
    })

  }
}
